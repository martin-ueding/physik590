% Copyright © 2014 Martin Ueding <dev@martin-ueding.de>

\chapter{Methoden}

\section{Harmonischer Oszillator}

Der harmonische Oszillator ist ein elementares quantenmechanisches System, das
durch den Hamiltonoperator
\[
    \hat H = \frac{1}{2m} \hat p^2 + \frac{m}{2} \omega^2 \hat x^2
\]
bestimmt wird \parencite[(3.1)]{Schwabl/Quantenmechanik}. Die Energieeigenwerte
lassen sich durch Operatormethoden am elegantesten berechnen. Dazu führe ich
die dimensionslose Länge
\[
    \xi := \sqrt{\frac{m\omega}{\hbar}} x
\]
ein. Im Ortsraum ist $\hat p = - \iup \hbar \pd{}x$. Der dimensionslose Impuls
ist demnach $\hat \pi^2 = - \hbar\omega m \pd[2]{}\xi$. Somit kann der
Hamiltonoperator umgeschrieben werden zu:
\[
    \hat H = \frac{\hbar\omega}2 \del{\xi^2 - \dpd[2]{}\xi}.
\]

Mit den Leiteroperatoren
\[
    \hat a := \frac1{\sqrt2} \del{\hat \xi + \iup \hat \pi}
    \quad\text{und}\quad
    \hat a^\dagger := \frac1{\sqrt2} \del{\hat \xi - \iup \hat \pi}
\]
kann der Hamiltonoperator kompakt als
\[
    \hat H = \hbar\omega \sbr{\hat a^\dagger \hat a + \frac12}
\]
geschrieben werden \parencite[(3.8)]{Schwabl/Quantenmechanik}. Dabei habe ich
schon den Kommutator $[\hat a, \hat a^\dagger] = 1$ ausgenutzt. Der Operator
$\hat a^\dagger \hat a$ ist der Beseztungszahloperator $\hat n$. In der
Energieeigenbasis $\{ \ket n \}_{n \in \N_0}$ sind die Eigenwerte von $\hat n$
durch $n$ gegeben, so dass die Energieeigenwerte
\[
    E_n = \hbar\omega \sbr{n + \frac 12}
\]
gegeben sind.

Im Ortsraum sind die Lösungen der Schrödungergleichung gegeben durch
\parencite[(4.159)]{nolting-theo5}:
\[
    \psi_n(\xi) = \del{\frac{m\omega}{\hbar\piup}}^{\frac14} (n! 2^n)^{-\frac12}
    \exp\del{- \frac{\xi^2}2} H_n(\xi),
\]
wobei $H_n$ die Hermite-Polynome sind. Im Grundzustand ist die Wellenfunktion
also gegeben durch:
\[
    \psi_0(\xi) = \piup^{-\frac14} \exp\del{-
    \frac{\xi^2}2}.
\]
Die zugehörige Aufenthaltswahrscheinlichkeit ist
\[
    |\psi_0(\xi)|^2 = \frac1{\sqrt\piup} \exp(-x^2).
\]

Die Wellenfunktion und Aufenthaltswahrscheinlichkeit gilt nur für das
Kontinuum. Für ein Zeitgitter ergeben sich andere Werte, dazu mehr in
Abschnitt~\ref{sec:wellenfunktion_auf_gitter}.

% TODO Abschnitt mit Gitter.

\section{Pfadintegral}

Feynmans Pfadintegralformalismus erlaubt es ein quantenmechanisches System mit
$n$ Freiheitsgraden in ein klassisches System mit $n+1$ Freiheitsgraden zu
überführen \parencite[§12.4]{Thijssen/Computational_Physics}. Dieses System
kann dann mit Methoden ähnlich der statistischen Physik behandelt werden.
Analog zur kanonischen Zustandssumme tritt hier eine Summe über alle
Weltlinien, die zwischen festem Anfangs- und Endpunkt $x_\text A$ bzw. $x_\text
E$ und einem Zeitinterval $T$ liegen, auf
\parencite[(2.7)]{Creutz/Statistical_Approach_QM}:
\[
    Z(x_\text E, x_\text A) = \sum_{\text{Trajektorien $j$}} \exp\del{ \iup
    \frac{S_j}{\hbar}}.
\]

Die Phase mit der Wirkung $S$ wird stark variieren, so dass es ab dieser Stelle
hilfreich ist, imaginäre Zeit zu betrachten. Dazu wird $\tau := \iup t$
eingeführt, was man auch \emph{Wick-Rotation} nennt. Diese hat die Eigenschaft,
ein quantenmechanisches System in ein Thermodynamisches zu überführen
\parencite[24]{Stetz/AQT}.

Schreibt man $\int [\dif x]$ für eine Integration über alle
Trajektorien $x(\tau)$, die $x(0) = x_\text A$ und $x(T) = x_\text E$ erfüllen,
so kann die Summe umgeschrieben werden zu
\parencite[(2.1)]{Creutz/Statistical_Approach_QM}:
\[
    \int [\dif x] \, \exp\del{-\frac{S(x)}\hbar}.
\]
$S$ ist hier ein Funktional der Trajektorie $x$. Dieser Integrand wird nur in
der Nähe der minimalen Wirkung Beiträge liefern. Dies reduziert den Bereich des
Phasenraums, über den integriert werden muss, derart, dass die Integration mit
Monte Carlo Methoden gut möglich ist.

Die Wirkung ist auch hier die Zeitintegration der Lagrangefunktion
\parencite[(2.5)]{Creutz/Statistical_Approach_QM}:
\[
    S = \int_0^T \dif \tau \, \sbr{\frac 12 m \sbr{\dpd x\tau(\tau)}^2 + V(x)}
\]

Diskretisiert man die Weltlinien, so wird es möglich, die Integration über alle
Weltlinien auszuführen. Das Integrationsmaß wird bei einer Unterteilung in
$\timesites+1$ (also $0, \ldots, \timesites$) Zeitpunkte zu:
\[
    \int [\dif x]
    \quad\leadsto\quad
    \prod_{j = 1}^{\timesites-1} \int_{-\infty}^\infty \dif x_j.
\]
Der Abstand der diskreten Zeitpunkte ist der Einfachheit halber konstant und
wird als $\timestep$ definiert. Somit gilt $T = \timesites\timestep$. Durch die
Diskretisierung werden Weltlinien unterdrückt, die sich zwischen den
Zeitpunkten stark ändern. Im Gegensatz zur reellen Zeit, in der Weltlinien mit
beliebiger Wirkung in die Zustandssumme einfließen können, fließen mit
imaginärer Zeit Weltlinien mit großer Geschwindigkeit (und daher großer
Wirkung) exponentiell weniger stark ein. Daher ist die Benutzung eines
Zeitgitters in Kombination mit imaginärer Zeit legitim. Im Grenzfall $\timestep
\to 0$ erhält man den kontinuierlichen Fall zurück.

Durch die diskrete Zeit wird in der Wirkung die Zeitableitung durch einen
Differenzenquotient ersetzt. Die Lagrangefunktion hängt nur noch von zwei
aufeinanderfolgenden Koordinaten ab. Die Wirkung einer Weltlinie $\vec x_k$
wird dann gegeben durch
\parencite[(3.2)]{Creutz/Statistical_Approach_QM}:
\[
    S(\vec x_k) = \timestep \sum_{j = 1}^\timesites L\del{x_j^{(k)}, x_{j+1}^{(k)}},
\]
wobei periodische Randbedingungen mit $x_{\timesites} = x_0$ festgesetzt wurden.

\section{Integration über den Phasenraum}

Der Raum der möglichen diskreten Weltlinien hat $\timesites$ Dimensionen und
ist unendlich groß. Aufgrund der hohen Dimension sind Monte Carlo Methoden zu
bevorzugen. Der Bereich der Weltlinien, die einen nennenswerten Beitrag zum
Integral liefern, ist jedoch aufgrund der exponentiellen Gewichtung recht
klein. Der Algorithmus von Metropolis wählt die Weltlinien entsprechend dieser
Gewichtung so aus, dass ein ungewichteter Mittelwert dieser Weltlinien einen
Schätzwert für die gewünschte Größe gibt
\parencite[434]{Creutz/Statistical_Approach_QM}. Die Wahrscheinlichkeit, die
Trajektorie $\vec x_k$ zu erhalten, wird durch
\begin{equation}
    \label{eq:p_x_k}
    p(\vec x_k) = \frac{\exp(-S(\vec x_k))}{\int [\dif x] \, \exp(-S(\vec x))}
\end{equation}
gegeben \parencite[(3.6)]{Creutz/Statistical_Approach_QM}. Werden $\iterations$
Weltlinien durch den Metropolisalgorithmus generiert, so ist der Schätzwert
$\overline A$ für
eine Größe $\bracket A$
\parencite[(3.7)]{Creutz/Statistical_Approach_QM}:
\[
    \overline A := \frac1\iterations \sum_{k=1}^\iterations A(\vec x_k).
\]

\subsection{Markovketten}

Um die Weltlinien entsprechend gewichtet generieren zu können, kommt ein
bestimmter Markovprozess zum Einsatz, der aus einer Weltlinie die nächste
generiert. Für $\iterations \to \infty$ nähert sich die Verteilung der
Benötigten an. \parencite[434]{Creutz/Statistical_Approach_QM}

% Eigenschaften von Markovketten

Die Weltlinien des einen Oszillators, die aus den $\timesites$ Koordinaten
$\{x_j\}$ bestehen, können auch als Koordinaten für $\timesites$ Teilchen
interpretiert werden. Somit ist eine Trajektorie ein Zustand dieses
Vielteilchensystems. Man kann nun eine Übergangsmatrix $W_{ij}$ definieren, die
die Wahrscheinlichkeit angibt, dass das System vom Zustand $i$ in den Zustand
$j$ wechselt. Aufgrund der diskreten Zeit entspricht ein Zeitschritt einem
Übergangsschritt.

% FIXME Hier klarer machen, was die x indiziert und was ganze Trajektorien
% indiziert.

Die Matrixelemente müssen $W_{ij} \geq 0$, sowie muss $\sum_j W_{ij} = 1$
gelten, da jeder Zustand in einen nächsten (oder sich selbst) übergehen muss
\parencite[(3.8)]{Creutz/Statistical_Approach_QM}. Da die $x_j$ aus $\R$
stammen, muss die Übergangsmatrix eine Wahrscheinlichkeitsdichte sein. Die
analogen Bedingungen sind dann $W(x_i, x_j) \geq 0$ und $\int \dif x_j \,
W(x_i, x_j) = 1$ für alle $x_i$
\parencite[(3.9)]{Creutz/Statistical_Approach_QM}.

% Verkettung von Schritten
% Eigenvektor, Wahrscheinlichkeitsdichte
% Forderungen an W

Die Übergangswahrscheinlichkeit über einen Zwischenschritt wird durch
Integration über die Zwischenposition,
\[
    W^{(2)}(x_i, x_k) = \int \dif x_j \, W(x_i, x_j) W(x_j, x_k),
\]
ermittelt \parencite[(3.9)]{Creutz/Statistical_Approach_QM}. Daraus lässt die
Rekursionsformel
\[
    W^{(n)}(x_i, x_k) = \int \dif x_j \, W^{(n-1)}(x_i, x_j) W(x_j, x_k),
\]
konstruieren \parencite[(3.10)]{Creutz/Statistical_Approach_QM}. Aufgrund der
Eigenschaften von $W$ lässt sich zeigen\footnote{Siehe
    \parencite[435]{Creutz/Statistical_Approach_QM} und
\parencite[Anhang~B]{Creutz/Statistical_Approach_QM}.}, dass es eine
Zustandsverteilungsdichte $P$ gibt, so dass sie linker Eigenvektor von
$W^{(\infty)}$ ist. Das $W$, das für die Pfadintegrale benötigt wird, soll gerade
$p(x)$ aus \eqref{eq:p_x_k} als Eigenvektor haben. Somit sind die
Anforderungen an $W$ die folgenden \parencite[(3.18)]{Creutz/Statistical_Approach_QM}:
\begin{itemize}
    \item
        $W(x_i, x_j) > 0$ falls $p(x_i) > 0$ und $p(x_j) > 0$,
    \item
        $\int \dif x_j \, W(x_i, x_j) = 1$,
    \item
        $p(x_j) = \int \dif x_i \, p(x_i) W(x_i, x_j)$.
\end{itemize}

% Detailed Balance

Um die Bedingungen zu erfüllen, ist die Wahl von $W$ so, dass
\[
    \frac{W(x_i, x_j)}{W(x_j, x_i)} = \frac{p(x_j)}{p(x_i)}
\]
gilt, möglich \parencite[(3.23)]{Creutz/Statistical_Approach_QM}. In der Quelle
wird die Relation „detailed balance condition“ genannt. Dieses
\emph{detailiertes Gleichgewicht} sagt aus, dass es sich bei $p$ um einen
Gleichgewichtszustand handelt, da Anzahl der Übergänge von $x_i$ nach $x_j$
genauso groß sind wie in der Rückrichtung
\parencite[85]{Schwabl/Quantenmechanik_fuer_Fortgeschrittene}.

Im Algorithmus geht nicht die ganze Trajektorie $\vec x$ in einem Schritt auf
eine neue Trajektorie $\vec x'$ über. Vielmehr werden die $x_j$ einzeln in ein
$x_j'$ überführt. Nach $N$ dieser Einzelschritte ist eine neue Weltlinie $\vec
x'$ erzeugt. Einer dieser Einzelschritte führt die Weltlinie $\vec x$ in eine
Weltlinie $\tilde{\vec x}$ über. Dabei wird nur die Koordinate $x_j$ zu $x_j'$
geändert. Mit \eqref{eq:p_x_k} kann man daher die Beziehung
\parencite[(3.25)]{Creutz/Statistical_Approach_QM}
\[
    \frac{W(\vec x, \tilde{\vec x})}{W(\tilde{\vec x}, \vec x)}
    = \frac{\exp(-S(x_j'))}{\exp(-S(x_j))},
\]
herleiten, welche durch die Wahl $W(x_j, x_j') \propto \exp(-S(x_j'))$ zu
erfüllen ist \parencite[(3.27)]{Creutz/Statistical_Approach_QM}. Dieser
Algorithmus wird „heat bath algorithm“ bezeichnet und hat den Nachteil, dass
es schwer ist, $x_j'$ entsprechend der Verteilung $\exp(-S(x_j'))$ zu ziehen
\parencite[438]{Creutz/Statistical_Approach_QM}.

\subsection{Metropolis Algorithmus}

Der Algorithmus von Metropolis et al.\ umgeht dieses Problem, indem er nach
einer einfach zu implementierenden Vorschrift, die $x_j'$ generiert
\parencite[439]{Creutz/Statistical_Approach_QM}, siehe
Algorithmus~\ref{alg:metropolis}.

\begin{algorithm}
    \begin{algorithmic}
        \For{$j \gets 1, \ldots, \timesites$}
            \State $x_j' \gets$ \Call{Zufallzahl}{$-\infty$, $\infty$}
            \State $\Deltaup S \gets S(x_0, \ldots, x_{j-1}, x_j',
            x_{j+1}, \ldots, x_\timesites) - S(\{x_i\})$

            \If{$\Deltaup S \leq 0$}
                \State $x_j \gets x_j'$
            \Else
                \State $r \gets$ \Call{Zufallszahl}{0, 1}
                \If{$r < \exp(-\Deltaup S)$}
                    \State $x_j \gets x_j'$
                \EndIf
            \EndIf
        \EndFor
    \end{algorithmic}
    % XXX Die zweite Zeile der Unterschrift ist nicht eingerückt.
    \caption{%
        Metropolisalgorithmus. Die Funktion $\textsc{Zufallzahl}(a, b)$ erzeugt
        eine Zufallszahl im Interval $(a, b)$.
    }
    \label{alg:metropolis}
\end{algorithm}

Es werden also neue Werte für $x_j$, die $x_j'$, aus einer Verteilung gezogen.
Ist die Änderung der Wirkung $\Deltaup S$ negativ, so wird der neue Wert
übernommen. Andernfalls wird der Wert mit der Wahrscheinlichkeit
$\exp(-\Deltaup S)$ übernommen. Wenn der Abstand zwischen $x_j'$ und $x_j$ groß
ist, ist die Wahrscheinlichkeit, dass der Wert übernommen wird, sehr gering.
Daher ist es effizienter, $x_j'$ nicht aus dem Intervall $(-\infty, \infty)$ zu
ziehen, sondern eine Breite $\Delta$ einzuführen
\parencite[439]{Creutz/Statistical_Approach_QM}. Die Autoren schlagen eine
Einheitsverteilung im Bereich $[x_j - \margin, x_j + \margin]$ vor. Ich benutze
in meiner Implementierung eine Normalverteilung mit einer Standardabweichung
$\margin$. Dies führt zu noch höheren Akzeptanzraten für neue Werte, die die
Wirkung $S$ erhöhen. Um den Übergang zu einer neuen Konfiguration noch weiter
zu beschleunigen, kann man das Ziehen eines neuen $x_j'$ mehrfach für das
gleiche $j$ wiederholen. Dadurch ist es möglich, dass es sich stärker
verändert, als in einem einzigen Schritt. Die Anzahl dieser Wiederholungen
möchte ich mit $\rounds$ bezeichnen.

\citeauthor{Creutz/Statistical_Approach_QM} geben
\[
    W(x_j, x_j') = \frac1{N_0} \del{
        \Theta(-\Deltaup S) + \exp(- \Deltaup S) \Theta(\Deltaup S)
        + \int \dif x' \, \del{
            1 - \exp(-\Deltaup S)
        } \Theta(\Deltaup S) \delta(x_j' - x_j)
    }
\]
als Wahrscheinlichkeitsdichte für diesen Algorithmus an
\parencite[(3.28)]{Creutz/Statistical_Approach_QM}, wobei ich $\Deltaup S :=
S(x_j') - S(x_j)$ definiert habe, $\Theta$ die Heaviside Stufenfunktion und
$N_0$ das Volumen des Konfigurationsraumes ist. Mit dieser Gleichung kann
nachvollzogen werden, dass $W$ die geforderten Bedingungen erfüllt.

\section{Bootstrap Methode}

% TODO Bootstrap Methode beschreiben.

Der Fehler zu einem statistischen Schätzwert ist in der Regel nicht einfach
anzugeben. Für Größen wie den Mittelwert ist es unter Annahme einer
zugrundeliegenden Verteilung möglich, nicht jedoch für komplexere Größen. Eine
Möglichkeit, eine Fehlerabschätzung zu beliebigen Schätzwerten zu erhalten, ist
die \emph{Bootstrap Methode}. Bei dieser werden aus der gegebenen Probe neue
Proben erstellt und eine Metaanalyse durchgeführt.

Sei $X$ eine Zufallsgröße, $\bracket X$ der Erwartungs- und $\overline X$ der
Schätzwert. Aus einer Stichprobenmenge $\mathbb M$ kann nun die Zufallsgröße
berechnet werden: $\overline X = X(\mathbb M)$. Im Grenzwert erhält man den
Erwartungswert, also $\lim_{|\mathbb M| \to \infty} \overline X = \bracket X$.

Aus der wirklich erhobenen Stichprobenmenge erzeugt man nun $\bootstrapsamples$
weitere Mengen $\mathbb M_i$ durch Ziehen mit Zurücklegen. Für jede dieser
Mengen errechnet man die Zufallsgröße und erhält die Menge der Schätzwerte
\[
    \mathbb X = \{ X(\mathbb M_i) \colon 1 \leq i \leq N_\text P \}.
\]
% TODO Quellenangabe für „normalverteilt“.
Diese Schätzwerte sind annähernd normalverteilt. Somit ist der so errechnete
Erwartungswert $\overline X$ durch den Mittelwert von $\mathbb X$ gegeben. Der
Fehler $\Deltaup \overline X$ ist durch die Standardabweichung von $\mathbb X$
gegeben.

\section{GEVP-Methode}

Beim harmonischen Oszillator kann die Grundzustandsenergie über den Virialsatz
bestimmt werden \parencite[(4.13)]{Creutz/Statistical_Approach_QM}:
\[
    E_0 = \mass^2 \bracket{x^2}.
\]
Die Größe $\bracket{x^2}$ ist in den noch zu definierenden
Korrelationsfunktionen als $C_{11}(0)$ enthalten.

Die Energie des ersten angeregten Zustands kann mit der Korrelation
berechnet werden \parencite[(4.14)]{Creutz/Statistical_Approach_QM}:
\begin{equation}
    \label{eq:Creutz_4.14}
    E_1 = \lim_{\tau \to \infty} \frac{-1}{\Deltaup \tau} \ln
    \del{\frac{\Bracket{x(0) \, x(\tau + \Deltaup \tau)}}{\Bracket{x(0) \,
    x(\tau)}}} + E_0.
\end{equation}

Dies ist äquivalent zu einer abfallenden Exponentialfunktion, die an die
Korrelationsfunktion $C_{11}(\tau) = \bracket{x(0) \, x(\tau)}$ angepasst wird.
Sei die Anpassungsfunktion
\[
    f(\tau) := c_1 \exp\del{-\frac{\tau}{c_2}}.
\]
Die Korrelationen in \eqref{eq:Creutz_4.14} drücke ich durch $f$ aus. Somit
erhalte ich:
\[
    E_1 - E_0
    = \lim_{\tau \to \infty} \frac{-1}{\Deltaup \tau} \ln
    \del{\frac{f(\tau + \Deltaup \tau)}{f(\tau)}}
    = \frac{1}{c_2}.
\]

Energien für noch höhere Zustände können mit dem „Generalized Eigenvalue
Problem“ (GEVP) errechnet werden.

\subsection{Interpolationsfelder}

Dazu werden Interpolationsfelder $O_i$ gewählt
\parencite[1]{Blossier/Eigenvalue}. In diesem Fall ist die Wahl $O_i(x) = x^i$
sinnvoll.
% TODO Quellenangabe / Danksagung

% TODO Erklärung, dass die Potenzen von x den Absteigeoperator enthalten und
% somit nur die gewünschten Zustände dabei herauskommen.
$x$ lässt sich durch die Ab- und Aufsteigeoperatoren $a$ bzw. $a^\dagger$
ausdrücken: $x \propto a + a^\dagger$
\parencite[(3.5a)]{Schwabl/Quantenmechanik}. Die Potenzen von $x$ enthalten
demnach Potenzen von $a$, womit die Matrixelemente $\braket{0|a^i|n} = 0$ sind,
wenn $i < n$. Sie sind jedoch ebenfalls 0, wenn $i - n$ ungerade ist. Die
größte Potenz von $a$ in $x^i$ ist $a^i$. Bei einer geraden Anzahl an Auf- und
Absteigevorgängen kann es insgesamt keine ungerade Änderung der Quantenzahl $n$
geben. Daher tragen für ein Matrixelement mit einer geraden Potenz von $x$ auch
nur gerade Energieeigenzustände bei.

\subsection{Korrelationsfunktionen}

Mit den Interpolationsfeldern $O_i$ lassen sich die Korrelationsfunktionen
\[
    C_{ij}(t) := \Bracket{O_i(t) \, O_j^*(t)}
    = \Bracket{x^i(0) \, x^j(t)}
\]
definieren \parencite[(2.1)]{Blossier/Eigenvalue}.

Da $\bracket{x} = 0$ aufgrund der Symmetrie des Potentials zu erwarten ist,
werden die Mittelwerte über ungerade Größen, also mit $i + j$ ungerade,
annähernd verschwinden. Es ist daher nicht sinnvoll, diese Größen überhaupt zu
berechnen.

Um das System numerisch stabiler zu bekommen, wird die Korrelationsmatrix in
einen nur geraden und einen nur ungeraden Teil zerlegt, also beide $i$ und $j$
gerade bzw. ungerade. Da wie vorher beschrieben auch nur gerade bzw. ungerade
Energieeigenzustände beitragen, kann man so sogar gerade und ungerade Zustände
trennen.

Das generalisierte Eigenwertproblem, aus dessen Eigenwerte die Energien
extrahiert werde können, ist durch
\[
    \tens C(t) \, \vec v_n(t, t_0) = \lambda_n(t, t_0) \, \tens C(t_0) \, \vec v_n(t,
    t_0)
    \eqnsep
    n = 1, \ldots, N
    \eqnsep
    t > t_0
\]
gegeben \parencite[(2.1)]{Blossier/Eigenvalue}. Aus den Eigenwerten lassen sich
die effektiven Energien
\[
    E_n^\text{eff}(t, t_0) = - \partial_t \ln\del{\lambda_n(t, t_0)} \equiv -
    \frac 1\timestep \sbr{ \ln\del{\lambda_n(t+\timestep, t_0)} -
    \ln\del{\lambda_n(t, t_0)} }
\]
bestimmen \parencite[(2.4)]{Blossier/Eigenvalue}, die mit $t \to \infty$ gegen
die Energie $E_n$ konvergieren \parencite[(2.3)]{Blossier/Eigenvalue}.

\subsection{Lösung des GEVP durch Cholesky-Zerlegung}

Das generalisierte Eigenwertproblem $\tens A \vec v = \lambda \tens B \vec v$
kann durch Multiplikation mit $\tens B^{-1}$ von links in ein normales
Eigenwertproblem überführt werden, wobei jedoch Symmetrien von $\tens A$ und
$\tens B$ verloren gehen. Um dies zu vermeiden, kann man das Problem mit der
Cholesky-Zerlegung lösen. Diese zerlegt $\tens B$ in eine untere Dreiecksmatrix
$\tens L$, so dass $\tens B = \tens L \tens L^\dagger$ gilt. Vorraussetzung
ist, dass $\tens B$ positiv definit ist, was hier der Fall ist.
% TODO Warum ist die positiv definit?
\parencite{MacKinnon/GEVP}


Damit kann das Problem umgeformt werden zu \parencite[(3.31)]{MacKinnon/GEVP}
\[
    \sbr{\tens L^{-1} \tens A [\tens L^\dagger]^{-1}} \tens L^\dagger \vec x
    =
    \lambda [\tens L^\dagger \vec x].
\]
Definiert man nun
\[
    \tilde{\tens A} := \sbr{\tens L^{-1} \tens A [\tens L^\dagger]^{-1}}
    \eqnsep
    \tilde{\vec x} := \tens L^\dagger \vec x,
\]
lässt sich dies als normale Eigenwertproblem schreiben \parencite[(3.32)]{MacKinnon/GEVP}:
\[
    \tilde{\tens A} \tilde{\vec x} = \lambda \tilde{\vec x}.
\]

Für die Implementierung der Cholesky-Zerlegung benutze ich die Funktion
\texttt{Eigen::MatrixBase::llt()} \parencite{Eigen/LLT} aus der C++ Bibliothek
Eigen. Das normale Eigenwertproblem löse ich anschließend mit der Klasse
\texttt{Eigen::EigenSolver} \parencite{Eigen/EigenSolver}.

\section{Eindimensionale Wellenfunktion mit $\delta$-Störung}

% TODO Einleitung für diesen Teil schreiben.

\newcommand\br[1]{\parencite[(#1)]{Busch/Two_Cold}}

In \citetitle{Busch/Two_Cold} leiten die Autoren eine analytische Lösung für
einen anharmonischen Oszillator her, bei welchem der Störterm eine
Deltadistribution ist. Damit berechnen sie dann Wellenfunktionen und
Energiewerte.

Die Herleitung für den eindimensionalen Fall wird als analog zum
dreidimensionalen Fall beschrieben. Anstelle des dreidimensionalen Potentials
$\sqrt 2 \piup a_0 \delta^{(3)}(\vec r) \pd{}r r$ soll einfach nur $V(x_1 -
x_2) = - 2/a_0 \delta(x_1 - x_2)$ benutzt werden
\parencite[Fußnote~20]{Busch/Two_Cold}.

Ich beginne meine Herleitung nach der Separation von Schwerpunkts- und
Relativbewegung analog zu \br2. Der Hamiltonoperator für den anharmonischen
Oszillator ist
\[
    H_\text{rel} = \underbrace{- \frac12 \nabla_x^2 + \frac 12
    x^2}_{H_\text{osz}} - \frac2{a_0} \delta(x).
\]

Dieser Operator löst (analog \br3) die zeitunabhängige Schrödingergleichung
\begin{equation}
    \label{eq:Bus_3}
    \del{H_\text{osz} - \frac2{a_0} \delta(x)} \Psi(x) = E \Psi(x).
\end{equation}

Die Autoren entwickeln die unbekannte Wellenfunktion $\Psi$ nun nach den
bekannten Wellenfunktionen des harmonischen Oszillators \br4:
\[
    \Psi(x) = \sum_{n=0}^\infty c_n \phi_n(x).
\]

Im eindimensionalen Fall gibt es keinen Drehimpuls $l$. Aber $\phi_n$ mit
ungeradem $n$ tragen nicht bei, da diese bei $x = 0$ einen Nulldurchgang haben.
Somit bleiben in der Summe nur die geraden $n$.


Dies setze ich (analog zu \br5) in die Schrödingergleichung \eqref{eq:Bus_3}
ein:
\[
    \del{H_\text{osz} - E - \frac2{a_0} \delta(x)}
    \sum_{n=0}^\infty c_n \phi_n(x)
    = 0
    \iff
    \sum_{n=0}^\infty (E_n - E) c_n \phi_n(x)
    \frac2{a_0} \delta(x)
    \sum_{n=0}^\infty c_n \phi_n(x)
    = 0
\]

Dies wird nun auf $\phi_m^*$ projiziert, i.\,e. Multiplikation von rechts und
Integration über $x$:
\[
    \int_{-\infty}^\infty \dif x \, \sum_{n=0}^\infty
    (E_n - E) c_n \phi_m^*(x) \phi_n(x)
    - 
    \int_{-\infty}^\infty \dif x \,  \frac2{a_0} \phi_m^*(x) \delta(x)
    \sum_{n=0}^\infty c_n \phi_n(x) = 0
\]

Mit der Orthogonalitätsrelation für die Eigenfunktionen
\[
    \int_{-\infty}^\infty \dif x \phi_m^*(x) \phi_n(x) = \braket{m|n} =
    \delta_{mn}
\]
folgt für die Schrödingergleichung
\[
    (E_m - E) c_m - \frac2{a_0} \phi_m^*(0) \sum_{n=0}^\infty c_n \phi_n(0) =
    0.
\]

Über den Index $n$ wird summiert, dieser ist außerdem für alle $m$ gleich.
Somit kann dieser Teil in eine Konstante $A$ verschoben werden. Daraus folgt
der Ansatz
\[
    c_m = A \frac{\phi_m^*(0)}{E_m - E}
\]
analog zu \br7. Diesen Ansatz setze ich in die Gleichung ein und erhalte:
\[
    \sum_{n=0}^\infty \frac{\phi_n^*(0) \phi_n(0)}{E_n - E} = \frac{a_0}2.
\]

Als nächsten Schritt werden die Eigenfunktionen des harmonischen Oszillators
$\phi_n$ eingesetzt. An der Stelle $x = 0$ verschwindet der Exponentialterm:
\[
    \sum_{n=0}^\infty \frac{1}{2^n n! \sqrt\piup} \frac{H_n^*(0) H_n(0)}{E_n -
    E} = \frac{a_0}2
\]

Mit dem Wert der Hermitepolynome am Ursprung,
\[
    H_n(0) = \frac{2^n \sqrt\piup}{\Gamma\del{\frac{1-n}2}},
\]
folgt:
\[
    \sum_{n=0}^\infty \frac{2^n \sqrt\piup}{n!} \frac{1}{E_n -
    E} \Gamma\del{\frac{1-n}2}^{-2} = \frac{a_0}2
\]

In die obige Gleichung setze ich die bekannten Energiewerte $E_n = n + 1/2$ für
den harmonischen Oszillator ein:
\[
    \sum_{n=0}^\infty \frac{2^n \sqrt\piup}{n!} \frac{1}{\frac 12 + n -
    E} \Gamma\del{\frac{1-n}2}^{-2} = \frac{a_0}2
\]

Mit Mathematica lässt sich diese Summe berechnen, wenn man nur über die geraden
$n$ summieren lässt:
\[
    \frac{2 \Gamma\del{\frac 14(5-2E)}}{(2E-1)\Gamma\del{\frac 14(3-2E)}}
    = \frac{a_0}2.
\]
Ausgedrückt mit der Kopplungsstärke $1/a_0$ ist die Relation
\[
    \frac{(2E-1)\Gamma\del{\frac 14(3-2E)}}{4 \Gamma\del{\frac 14(5-2E)}}
    = \frac1{a_0}.
\]


Dies ist in Abbildung~\ref{fig:E_a0} dargestellt.

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{E_a0}
    \begin{tikzpicture}
        \begin{axis}[
                %grid=major,
                width=\linewidth,
                height=0.6\linewidth,
                xmin=-5,
                xmax=5,
                xlabel=$1/a_0$,
                ylabel=$E$,
                xtick={-1,0,1},
                ytick={0.5,1.5,...,9.5},
                grid=major,
            ]
            \addplot[black] table {_build/gamma_data.txt};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Energie $E$ in Abhängigkeit von der Kopplungsstärke $1/a_0$.
    }
    \label{fig:E_a0}
\end{figure}

%\nocite{Thijssen/Computational_Physics}

%\nocite{Young/Jackknife}
%\nocite{Oser/Bootstrap}

%\nocite{Stroustrup/C++4}

% vim: ft=tex spell spelllang=de tw=79
% Copyright © 2014 Martin Ueding <dev@martin-ueding.de>

\chapter{Implementierung}

% FIXME Dies irgendwie weniger aufdringlich unterbringen.
\begin{small}
    Mein Programm ist freie Software und unter
    \url{http://martin-ueding.de/de/university/physik590/} erhältlich.
\end{small}

% TODO Geschickte Berechnung der Änderung in der Entropie.

Die Weltlinie wird in der Regel anfangs mit zufälligen Werten für die $x_j$ aus
dem Intervall $(-\initialrandomwidth, \initialrandomwidth)$ initialisiert. Bevor
Werte aufgenommen werden, durchläuft das System $\preiterations$ Monte Carlo
Iterationen (meist \num{50}) zur Relaxation. Danach sieht die Weltlinie wie in
Abbildung~\ref{fig:relaxiert} aus.

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{relaxiert}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.5\linewidth,
                xlabel={$j = t/a$},
                ylabel={$x_j = x(t/a)$},
            ]
            \addplot[black] table {CSV/11EA8F-trajectory-04-more_iterations.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Weltlinie nach $\preiterations = \num{50}$ Iterationen zur Relaxation.
        Dabei sind die Parameter $\timestep = \num{0.1}$, $\initialrandomwidth
        = \margin = \num{0.632456}$, und $\mu^2 = \num{1}$.
    }
    \label{fig:relaxiert}
\end{figure}

\section{Harmonischer Oszillator}

\subsection{Aufenthaltswahrscheinlichkeit}

In Abbildung~\ref{fig:histogram_01} ist die Aufenthaltswahrscheinlichkeit für
eine recht feine Zeitauflösung $\timestep$ dargestellt. Die erwartete
Verteilung unterscheidet sich kaum von der Kontinuumslösung. Am Maximum der
Kurve ist noch am ehesten zu erkennen, dass das Kontinuum ein Bruchteil der
Liniendicke unterhalb der Gitterlösung liegt, siehe
Abbildung~\ref{fig:histogram_01_zoom}.

Bei einer geringeren Zeitauflösung $\timestep$ ist eine Normalverteilung mit
geringerer Breite zu erwarten. Für $\timestep = \num{1}$ ist die
Aufenthaltswahrscheinlichkeit in Abbildung~\ref{fig:histogram_10} dargestellt.
Die Datenpunkte aus der Simulation liegen gut auf der erwarteten Kurve. Diese
weicht merklich von der Kontinuumslösung ab.

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{histogram_01}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.55\linewidth,
                xlabel={$x$},
                ylabel={$|\psi(x)|^2$},
                grid=major,
                xmin=-3,
                xmax=3,
                legend entries={
                    {Simulation},
                    {Theorie},
                    Kontinuum,
                },
            ]
            \addplot[
                black,
                only marks,
                mark=|,
                error bars/.cd,
                y dir=both,
                y explicit
            ] table[y error index=2] {CSV/histogram-korrigiert.csv};
            \addplot[black] table {_build/histogram_theory_01.csv};
            \addplot[black, dashed] table {_build/histogram_theory_continous.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Aufenthaltswahrscheinlichkeit. $\timestep =
        \num{0.1}$, $\Delta = \num{0.632456}$, $\mu^2 = \num{1}$, $\timesites =
        \num{1000}$, $\iterations = \num{1000}$, $\bootstrapsamples = \num{10000}$.
    }
    \label{fig:histogram_01}
\end{figure}

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{histogram_01_zoom}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.55\linewidth,
                xlabel={$x$},
                ylabel={$|\psi(x)|^2$},
                grid=major,
                xmin=-0.3,
                xmax=0.3,
                legend entries={
                    {Simulation},
                    {Theorie},
                    Kontinuum,
                },
            ]
            \addplot[
                black,
                only marks,
                mark=|,
                error bars/.cd,
                y dir=both,
                y explicit
            ] table[y error index=2] {CSV/histogram-korrigiert.csv};
            \addplot[black] table {_build/histogram_theory_01.csv};
            \addplot[black, dashed] table {_build/histogram_theory_continous.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Ausschnittsvergrößerung der Aufenthaltswahrscheinlichkeit. $\timestep =
        \num{0.1}$, $\Delta = \num{0.632456}$, $\mu^2 = \num{1}$, $\timesites =
        \num{1000}$, $\iterations = \num{1000}$, $\bootstrapsamples = \num{10000}$.
    }
    \label{fig:histogram_01_zoom}
\end{figure}

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{histogram_10}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.55\linewidth,
                xlabel={$x$},
                ylabel={$|\psi(x)|^2$},
                grid=major,
                xmin=-3,
                xmax=3,
                legend entries={
                    {Simulation},
                    {Theorie},
                    Kontinuum,
                },
            ]
            \addplot[
                black,
                only marks,
                mark=|,
                error bars/.cd,
                y dir=both,
                y explicit
            ] table[y error index=2] {CSV/DC5256-histogram-position-resultset.csv};
            \addplot[black] table {_build/histogram_theory_1.csv};
            \addplot[black, dashed] table {_build/histogram_theory_continous.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Aufenthaltswahrscheinlichkeit. $\timestep =
        \num{1}$, $\margin = \num{0.632456}$, $\mu^2 = \num{1}$, $\timesites =
        \num{1000}$, $\iterations = \num{5000}$, $\bootstrapsamples = \num{1000}$.
    }
    \label{fig:histogram_10}
\end{figure}

\subsection{Korrelationen}


In einem Simulationsdurchlauf mit $\timestep = \num{0.1}$, $\timesites =
\num{10000}$ und $\iterations = \num{10000}$ erhalte ich gute Korrelationen bis
$\tau \approx \num{7}$, siehe Abbildung~\ref{fig:84DAA2-correlation}. Aus der
Anpassung an diese Daten erhalte ich $1/c_2 = E_1 - E_0 = \num{0.995 +-
0.002}$. Dies passt gut mit der dem Kontinuumsfall von $\Deltaup E_{0,1} =
\num{1}$ zusammen.

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{84DAA2-correlation}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.55\linewidth,
                xlabel={$\tau$},
                ylabel={$C_{11}(\tau)$},
                grid=major,
                legend entries={
                    {Simulation},
                    {Anpassung},
                },
            ]
            \addplot[
                black,
                only marks,
                mark=|,
                error bars/.cd,
                y dir=both,
                y explicit
            ] table[y error index=2] {_build/84DAA2-data.csv};
            \addplot[black] table {_build/84DAA2-fit.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Korrelationsfunktion $C_{11}(\tau) = \bracket{x(0) \, x(\tau)}$ gegen
        imaginäre Zeit $\tau$. An die Punkte wurde eine abfallende
        Exponentialfunktion angepasst. Die Daten stammen aus einer Simulation
        mit $\timestep = \num{0.1}$, $\initialrandomwidth = \margin =
        \num{0.632456}$, $\mu^2 = \num{1}$, $\timesites = \num{10000}$,
        $\iterations = \num{10000}$ und $\bootstrapsamples = \num{1000}$.
    }
    \label{fig:84DAA2-correlation}
\end{figure}

% TODO Höhere Energiezustände mit GEVP Methode.

\section{Anharmonischer Oszillator}

Ab hier wird dem harmonischen Potential ein $\delta$-Potential überlagert. In
der Simulation wird dies durch schmale Normalverteilungen angenähert. Deren
Integral ist auf eins normiert und die Verteilungen haben eine Breite
$\gausswidth$. Je nach Breite ergibt sich ein anderes Potential. In
Abbildung~\ref{fig:gausspotential-positiv} ist das Gesamtpotential für eine
positive Kopplungsstärke $1/a_0$ gezeigt.
Abbildung~\ref{fig:gausspotential-negativ} enthält das Potential für eine
negative Kopplungsstärke.

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{gausspotential-positiv}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.55\linewidth,
                xlabel={$x$},
                ylabel={$V(x)$},
                grid=major,
            ]
            \addplot[black] table {CSV/gauss-potential/pos/00C58F-potential.csv};
            \addplot[black] table {CSV/gauss-potential/pos/18B9E2-potential.csv};
            \addplot[black] table {CSV/gauss-potential/pos/53D7EB-potential.csv};
            \addplot[black] table {CSV/gauss-potential/pos/6FC501-potential.csv};
            \addplot[black] table {CSV/gauss-potential/pos/DF05AB-potential.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Überlagerung von harmonischem Potential mit Gaußpotential für
        verschiedene Breiten der Gaußfunktion $\gausswidth$. $\mu^2 = \num{1}$,
        $1/a_0 = \num{1}$.
    }
    \label{fig:gausspotential-positiv}
\end{figure}

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{gausspotential-negativ}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.55\linewidth,
                xlabel={$x$},
                ylabel={$V(x)$},
                grid=major,
            ]
            \addplot[black] table {CSV/gauss-potential/neg/029833-potential.csv};
            \addplot[black] table {CSV/gauss-potential/neg/156D90-potential.csv};
            \addplot[black] table {CSV/gauss-potential/neg/4DD6F0-potential.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Überlagerung von harmonischem Potential mit Gaußpotential für
        verschiedene Breiten der Gaußfunktion $\gausswidth$. $\mu^2 = \num{1}$,
        $1/a_0 = \num{-1}$.
    }
    \label{fig:gausspotential-negativ}
\end{figure}

Mit dem veränderten Potential können alle vorherigen Rechnungen wiederholt
werden. Abbildung~\ref{fig:histogram_gauss} zeigt die
Aufenthaltswahrscheinlichkeit.

\begin{figure}[htbp]
    \centering
    \tikzsetnextfilename{histogram_gauss}
    \begin{tikzpicture}
        \begin{axis}[
                width=\linewidth,
                height=0.55\linewidth,
                xlabel={$x$},
                ylabel={$|\psi(x)|^2$},
                grid=major,
            ]
            \addplot[
                black,
                only marks,
                mark=|,
                error bars/.cd,
                y dir=both,
                y explicit
            ] table[y error index=2] {CSV/FF7AA0-histogram-position-resultset.csv};
        \end{axis}
    \end{tikzpicture}
    \caption{%
        Aufenthaltswahrscheinlichkeit. $\timestep = \num{0.1}$,
        $\initialrandomwidth = \num{0}$, $\margin = \num{0.632456}$, $\mu^2 =
        \num{1}$, $\timesites = \num{1000}$, $\iterations = \num{10000}$,
        $\bootstrapsamples = \num{1000}$, $\gaussheight = \num{10}$,
        $\gausswidth = \num{1.0}$.
    }
    \label{fig:histogram_gauss}
\end{figure}

% vim: ft=tex spell spelllang=de tw=79
